{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "## Introduction\n",
    "K-means is an example of unsupervised learning algorithms for clustering problems.\n",
    "Suppose we have a data set $X=\\{ \\mathbf{x}_1 \\cdots \\mathbf{x}_N \\}$ consisting of $N$ observations of a random $D$-dimensional Euclidean variable $\\mathbf{x}$. Our goal is to partition the data set into some number $K$ of clusters, located around their centroids $\\mathbf{m}_k=\\{\\mathbf{\\mu}_1 \\cdots \\mathbf{\\mu}_K \\}$. We assume that the value of $K$ is given.\n",
    "\n",
    "K-means clustering can be solved by Expectation Maximisation (EM) algorithm which consists of two steps: E-step and M-step.\n",
    "On each E-step, we find the Euclidean distance between $N$ data points and $K$ cluster centers. The most probable cluster for each data sample $\\mathbf{x}_n$ is the one with nearest centroid:\n",
    "\n",
    "\\begin{align}\n",
    "z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2\n",
    "\\end{align}\n",
    "\n",
    "The M-step updates each cluster center by computing the mean of all points assigned to it:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n\n",
    "\\end{align}\n",
    "\n",
    "The pseudo-code of K-means clustering can be summarised as following:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "    Randomly select cluster centers ($\\mathbf{m}_k$) as initial centroids; <br/> \n",
    "<b>while</b> <i>centroids change</i> <b>do</b>:     \n",
    "&emsp; <b>E-step</b>: <br/> \n",
    "&emsp; Calculate the distance between each data point and cluster center (centroid); <br /> \n",
    "&emsp; Assign each data point to its closest cluster center (centroid): $z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2$; <br /> \n",
    "&emsp; Form K clusters by assigning each point to its closest centroid; <br /> \n",
    "<br/> \n",
    "&emsp; <b>M-step</b>: <br />\n",
    "&emsp; Update each cluster center by computing the mean of all points assigned to it: $\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n$; <br /> \n",
    "<b>end</b> <br /> \n",
    "<b>Result</b>: Cluster indices of each data point (assignments)    \n",
    "  </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: Clustering on a simple toy data\n",
    "In this subsection, we implement K-means to cluster a simple toy data. \n",
    "First, we import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We import all necessary libraries\n",
    "from pyKmeans import Kmeans # This imports the Kmeans function (which we created)\n",
    "import numpy as np # This imports numerical python (numpy) library\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a simple data based on two independent multivariate Gaussian distributions and we plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create synthetic data using two Gaussian distributions\n",
    "# First, we determine the mean and standard deviation of two Gaussian distributions\n",
    "mu1=np.array([5,2]); sigma1=np.array([[0.4,-0.0255],[-0.0255,0.2]])\n",
    "mu2=np.array([9,7]); sigma2=np.array([[0.10,0],[0,0.4]])\n",
    "\n",
    "# Second, we determine the number of data points on each Gaussian distribution\n",
    "N1=300; N2=100\n",
    "\n",
    "# Third, we add these properties into multivariate normal dist. function in numpy\n",
    "X1=np.random.multivariate_normal(mu1, sigma1, N1)\n",
    "X2=np.random.multivariate_normal(mu2, sigma2, N2)\n",
    "X = np.concatenate((X1, X2), axis=0) # combine X1 and X2 as data X\n",
    "\n",
    "# Fourth, we plot the synthetic data based on two Gaussian distribution function \n",
    "plt.plot(X[:,0],X[:,1],'bo',linewidth=2.0,markersize=4.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in the Figure shows very well separated data.\n",
    "Of course, it is very easy to visualize the separation/group by human eye, but clustering aims to do this process automatically. K-means clustering is one of simple algorithm to cluster/group data sets.\n",
    "\n",
    "In this tutorial, the number of clusters K is assumed to be known. There are some automatic\n",
    "methods in determining K parameter, but they are not discussed here.\n",
    "\n",
    "We then apply <i>K-means</i> function on the data (if you are interested the detailed of the algorithm, you can take a look at <i>Kmeans.py</i>). The function returns the cluster indices as well as the cluster's\n",
    "centroids. Finally, we plot the data for each cluster using different colour.\n",
    "\n",
    "The produced Figure demonstrates how K-means algorithm successfully forms two clusters in the data.\n",
    "The above problem of course is very easy to visualize because the generated data set is very well separated. The next exercise will demonstrate more challenging problem. The exercise is a good example to understand how K-means can be useful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part, we will apply K-means clustering algorithm to the above generated data\n",
    "\n",
    "K=2 # determine the number of cluster\n",
    "\n",
    "assignment1, mu_k = Kmeans(X,K)\n",
    "\n",
    "k1=np.argwhere(assignment1==1) # index of cluster 1\n",
    "k2=np.argwhere(assignment1==2) # index of cluster 2\n",
    "\n",
    "plt.plot(X[k1,0],X[k1,1],'ro',X[k2,0],X[k2,1],'go',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(mu_k[0,0],mu_k[0,1],'bx',mu_k[1,0],mu_k[1,1],'bx',linewidth=20.0,markersize=20.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Exercise:\n",
    "# Bring the closer the means of Gaussian distribution, what is the effect of K-means clustering\n",
    "# Play around with the generated dataset and see K-means effect\n",
    "# Here we use K=2, because we know we generated the data from two Gaussian distributions, what happened if you change K\n",
    "\n",
    "# Homework:\n",
    "# Understand the K-means clustering function\n",
    "# Change the distance metrics, from Euclidean distance to be Manhattan distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: K-means clustering on synthetic data\n",
    "### Exercise 1.a:\n",
    "As instructed earlier in the simple exercise above, vary the aforementioned parameters [mu1,mu2],[sigma1,sigma2] and [N1,N2] and see the plots. You may produce two groups which are not well separated (or at least close each others). Perform K-means clustering on that data.\n",
    "### Exercise 1.b:\n",
    "Create synthetic 3D-data using a multivariate Gaussian distribution, select appropraiately parameters [mu1,mu2,mu3],[sigma1,sigma2,sigma3] and [N1,N2,N3] and see the plots. Perform K-means clustering on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: K-means clustering on chemical wine data\n",
    "\n",
    "This exercise is a continuation of previous wine exercise (PCA). \n",
    "We will use the wine data set, described in the previous notebook.\n",
    "\n",
    "Based on the eigenvalues generated by PCA in the data, it is known that there are three most dominant PCs. \n",
    "The objective is to apply K-means on the reduced data (using the first three PCs) for clustering three different groups of wine. \n",
    "We know the actual classiffication of the wine from the data set into three types, so we hope that the clusters generated through K-means mimic the actual wine types. \n",
    "\n",
    "### Exercise 2.a:\n",
    "We provide the template for this exercise below.\n",
    "Perform K-means on the reduced data (using only three PCs). Produce a 3D plot of the grouped wine data based on clustering and compare the results with the actual wine classes.\n",
    "\n",
    "### Exercise 2.b:\n",
    "Perform K-means clustering without applying PCA on the data and compare the results with the exercise 2.a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all necessary libraries\n",
    "from pyPCA import PCA, zscore   # import the same functions made in the first notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "\n",
    "\n",
    "# We load the wine data again\n",
    "dataIn = np.genfromtxt('./data/wineInputs.txt', delimiter=',')\n",
    "dataOut = np.genfromtxt('./data/wineOutputs.txt', delimiter=',')\n",
    "dataOut=np.int_(dataOut)\n",
    "\n",
    "## # do the zscore and PCA of dataIn\n",
    "X = zscore(dataIn) # normalization\n",
    "\n",
    "# Using PCA.py function, apply PCA on the normalized data:\n",
    "[V, Ypca ,D] = PCA(X) # applying PCA\n",
    "\n",
    "\n",
    "# determine the number of cluster\n",
    "K = 3 # We assume that we know the number of cluster\n",
    "\n",
    "## Using Kmeans.py function, apply K-means clustering on the first three PCs data\n",
    "assignment1, mu_k = Kmeans(Ypca[:,0:2],K) # calculate clusters and centroids\n",
    "\n",
    "# Below is to re-show the cumulative sum of PCs\n",
    "noPC=np.linspace(1,len(D),len(D))\n",
    "idc=np.divide(np.cumsum(D),np.sum(D))\n",
    "plt.figure(1)\n",
    "plt.plot(noPC,idc,'bo') # re-plot the data\n",
    "plt.xlabel('no PC')\n",
    "plt.ylabel('cumsum')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plot the real classes here\n",
    "fig1 = pylab.figure()\n",
    "ax = Axes3D(fig1)\n",
    "ax.scatter(Ypca[dataOut==1,0],Ypca[dataOut==1,1],Ypca[dataOut==1,2],c='b')\n",
    "ax.scatter(Ypca[dataOut==2,0],Ypca[dataOut==2,1],Ypca[dataOut==2,2],c='r')\n",
    "ax.scatter(Ypca[dataOut==3,0],Ypca[dataOut==3,1],Ypca[dataOut==3,2],c='g')\n",
    "plt.title('Real Classes')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Plot the wine classes based on K-means clustering here\n",
    "fig2 = pylab.figure()\n",
    "ax = Axes3D(fig2)\n",
    "ax.scatter(Ypca[assignment1==1,0],Ypca[assignment1==1,1],Ypca[assignment1==1,2],c='b')\n",
    "ax.scatter(Ypca[assignment1==2,0],Ypca[assignment1==2,1],Ypca[assignment1==2,2],c='r')\n",
    "ax.scatter(Ypca[assignment1==3,0],Ypca[assignment1==3,1],Ypca[assignment1==3,2],c='g')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Kmeans')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: K-means clustering on atmospheric variables\n",
    "\n",
    "Atmospheric new-particle formation (NPF) is an important source of climatically\n",
    "relevant atmospheric aerosol particles. \n",
    "NPF is the episodes where the ultra-fine aerosol particles appearing in the atmosphere and growing subsequently to larger sizes until they reach sizes where they potentially scatter solar irradiation and impact cloud condensation nuclei (CCN).\n",
    "This fact has motivated scientists across the world to study the atmospheric variables which contribute to the process of NPF. Below Figure shows two examples of new-particle formation events, the Figures on the left and right hand sides constitute non-event and event days, respectively.\n",
    "\n",
    "<table>\n",
    "  <caption align=\"bottom\">Examples of non-event and event days at Hyyti{\\\"a}l{\\\"a}, SMEAR II, Finland, in May 2005.</caption>\n",
    "  <tr><td>\n",
    "    <th><img src=\"images/NonEventDay.png\" width=\"200px\"></th>\n",
    "    <th><img src=\"images/EventDay.png\" width=\"200px\"></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In this exercise, we have a data set consists of two daily averaged atmospheric variables: global radiation ($W/ms^{-2}$) and Relative Humidty (%). They were measured from January 1996 - December 2014 at Hyyti{\\\"a}l{\\\"a}, SMEAR II, Finland. \n",
    "The below figure shows the scatter plot between RH and Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy to load file of relative humidity and solar radiation: \n",
    "Data = np.loadtxt('./data/DataForClusteringFinal.txt', skiprows=1)\n",
    "\n",
    "# Relative Humidty (RH) and Global radiation to NPF\n",
    "# X is the data which we will use as a unsupervised learning\n",
    "# y is the labels which we will use later for comparison\n",
    "X = np.stack((Data[:,1],Data[:,2]),axis=1)\n",
    "y = np.stack((Data[:,3],Data[:,4]),axis=1)\n",
    "\n",
    "#idxNotNaN = ~np.isnan(X)\n",
    "idxNotNaN = ~np.isnan(X).any(axis=1)\n",
    "X = X[idxNotNaN,:]\n",
    "y = y[idxNotNaN,:]\n",
    "# Kmeans clustering algorithm\n",
    "#from sklearn.cluster import KMeans\n",
    "#clus = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "#idx = clus.labels_\n",
    "\n",
    "idx, mu_k = Kmeans(X,2) # calculate clusters and centroids\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(122)\n",
    "#plt.hold(True)\n",
    "#plt.plot(X[idx==0,0],X[idx==0,1],'b.')\n",
    "plt.plot(X[idx==2,0],X[idx==2,1],'b.')\n",
    "plt.plot(X[idx==1,0],X[idx==1,1],'r.')\n",
    "plt.title('K-means clustering')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 500)\n",
    "plt.xlabel('Relative Humidty (%)')\n",
    "plt.ylabel('Global Radiation (W/m^2)')\n",
    "#plt.hold(False)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "#plt.hold(True)\n",
    "# plot event:\n",
    "idxE=np.where(y[:,0]==1)\n",
    "plt.plot(X[idxE,0],X[idxE,1],'r.')\n",
    "# plot non-event:\n",
    "idxE=np.where(y[:,1]==1)\n",
    "plt.plot(X[idxE,0],X[idxE,1],'b.')\n",
    "plt.title('Real NPF cluster')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 500)\n",
    "plt.xlabel('Relative Humidty (%)')\n",
    "plt.ylabel('Global Radiation (W/m^2)')\n",
    "#plt.hold(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
