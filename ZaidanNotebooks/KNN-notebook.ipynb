{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor classification\n",
    "## Introduction\n",
    "In a classification problem, each input $\\mathbf{x}$ has a corresponding class label, $c^n \\in \\{1, \\cdots, C \\}$. \n",
    "Given a data set of $N$ training examples, $D=\\{\\mathbf{x}^n, c^n \\}, n=1, \\cdots, N$ and new test data $\\mathbf{x}_t$, the goal is predict the correct class $c_t(\\mathbf{x}_t)$.\n",
    "\n",
    "K-Nearest Neighbor (KNN) is commonly used for a classification algorithm. \n",
    "The Figure below presents an example of KNN classification.\n",
    "The test data point (green circle) can be classified either to the first class (blue squares) or to the second class (red triangles). If we use Nearest Neighbor algorithm, the test data point is assigned to be in the second class, because it has the closest distance to a red triangle.\n",
    "In KNN, $K$ is an extra paramater to be determined, that is the number of the nearest neigbor.\n",
    "If we select $K = 3$ (solid line circle), the test data is assigned to the second class because there are two red triangles and only one blue square inside the inner circle. However, if we select $K = 5$ (dashed line circle), the test data is assigned to the first class (three squares vs. two triangles inside the outer circle).\n",
    "\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src=\"images/KnnClassification.png\" width=\"400px\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "Example of K-Nearest Neighbor (KNN) classification.\n",
    "</td></tr>\n",
    "</table><br>\n",
    "\n",
    "The algorithm of KNN classification can be summarized as following:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "    <b>Step 1</b>: Determine paramater of $K$ as the number of nearest neigbors; <br/> \n",
    "    <b>Step 2</b>: Calculate the dissimilarity of the test point $\\mathbf{x}_t$ to each of the training data, $d(\\mathbf{x}_t,\\mathbf{x}^n)$, where $n=1, \\cdots , N$; <br/>\n",
    "    <b>Step 3</b>: Sort the distance and determine the nearest neigbors based on K-th minimum distance; <br/>\n",
    "    <b>Step 4</b>: Gather categories of the nearest neigbors; <br/>\n",
    "    <b>Step 5</b>: Assign the class label $c_t(\\mathbf{x}_t)$ using the simple majority of nearest neigbors as the prediction class; <br/> \n",
    "  </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: KNN classification on synthetic data\n",
    "In this tutorial, we attempt to classify a test point, either to the first group or the second group. All training data and also a test data are generated.\n",
    "Here, we will use the same synthetic data as in tutorial of K-means clustering.  But, in this case, we will label each group as output. \n",
    "\n",
    "First we import all necessary libraries and then we generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyKNN import KNN # This imports the KNN function (which we created)\n",
    "import numpy as np # This imports numpy\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting\n",
    "\n",
    "# First, we generate synthetic data (the same as Kmeans clustering data)\n",
    "\n",
    "# We create synthetic data using two Gaussian distributions\n",
    "# We determine the mean and standard deviation of two Gaussian distributions\n",
    "mu1=np.array([5,2]); sigma1=np.array([[0.4,-0.0255],[-0.0255,0.2]])\n",
    "mu2=np.array([9,7]); sigma2=np.array([[0.10,0],[0,0.4]])\n",
    "\n",
    "# We determine the number of data points on each Gaussian distribution\n",
    "N1=300; N2=100\n",
    "\n",
    "# We add these properties into multivariate normal dist. function in numpy\n",
    "X1=np.random.multivariate_normal(mu1, sigma1, N1)\n",
    "X2=np.random.multivariate_normal(mu2, sigma2, N2)\n",
    "X=np.concatenate((X1, X2), axis=0) # combine X1 and X2 as data X\n",
    "\n",
    "# Second, because KNN is a supervise learning algorithm, we need to label the above data\n",
    "Y=np.zeros((N1+N2,1)) # Output initiation\n",
    "Y[0:N1,0]=1  # The first data, we label as class 1\n",
    "Y[N1:N2,0]=2 # The second data, we label as class 2\n",
    "\n",
    "Data=np.concatenate((Y,X),1)\n",
    "\n",
    "# Third, we use all data as training data (we will give a testinput later)\n",
    "noTrain=N1+N2 # Determine the number of training points\n",
    "traindata=Data[0:noTrain-1,:]\n",
    "\n",
    "# Fourth, we plot the synthetic data based on two Gaussian distribution function \n",
    "plt.figure()",
    "plt.plot(X1[:,0],X1[:,1],'bs',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(X2[:,0],X2[:,1],'r^',linewidth=2.0,markersize=4.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Training Data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we produced training data, now we generate test data. \n",
    "We select one data point as a test point. We label the test data based on our visualization, to know which class this data point should belong to. Below we select test data point at coordinate [7,4] and we know through visualization (from the distance), it should be in the class 1 (blue squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xt=np.array([7,4]); # We select the test input coordinate\n",
    "Xt=np.transpose(Xt.reshape((np.size(Xt,axis=0), 1)))\n",
    "LabelTest=1 # From the above Figure, we assume we know that the above test point belongs to class 1\n",
    "Yt=np.array([LabelTest,]);\n",
    "Yt=np.transpose(Yt.reshape((np.size(Yt,axis=0), 1)))\n",
    "\n",
    "testdata=np.concatenate((Yt,Xt),1) # as training set, we concatenate test input and test output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training data and a test data point as well as their correspondent outputs.\n",
    "Next, we apply <i>KNN</i> function (remember that we imported this function earlier: \"<i>from pyKNN import KNN</i>\").\n",
    "\n",
    "Firstly, we need to select the number of nearest neigbor, $K$ snd then we pass arguments of training data and test data as well as $K$ into <i>KNN</i> function. \n",
    "This returns <i>expclass</i> and confusion matrix (<i>CM</i>). The <i>expclass</i> is the predicted class of test point. A confusion matrix is a table that is often used to describe the performance of a classification model. In this case, we do not print this, because we only have a test point. We will use the confusion matrix in the second exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K=3 # Determine the number of nearest neigbor\n",
    "\n",
    "expclass, CM = KNN(traindata, testdata, K) # Apply KNN\n",
    "\n",
    "testdata1=testdata[:,0]\n",
    "testdata1=testdata1.reshape((np.size(testdata1,axis=0), 1))\n",
    "\n",
    "Results = np.concatenate((testdata1,expclass),axis=1) \n",
    "print('TestData','KNN-ClassPrediction')\n",
    "print(Results)\n",
    "\n",
    "# We plot the synthetic data based on two Gaussian distribution function \n",
    "plt.figure()",
    "plt.plot(X1[:,0],X1[:,1],'bs',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(X2[:,0],X2[:,1],'r^',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(Xt[:,0],Xt[:,1],'go',linewidth=2.0,markersize=8.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: KNN classification on wine data\n",
    "\n",
    "In the second tutorial, we will apply KNN on chemical wine data. The chemical properties of wine data was described in the section of PCA.\n",
    "\n",
    "First we imports all necessary libraries and load the input and output data sets. Next, we randomize the data and divide the data to be training and testing data. Subsequently, we select the value of $K$ and we implement <i>KNN</i> function to return the predicted classes of test data and the confusion matrix (as classification peformance metric). At the end, we compare the results of predicted classes and actual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyKNN import KNN # This imports the KNN function (which we created)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# load the training data - do not edit\n",
    "wineTrainIn = np.load(\"./data/wine-training-input.npy\")\n",
    "wineTrainOut = np.load(\"./data/wine-training-output.npy\")\n",
    "\n",
    "# load the validation data - do not edit\n",
    "wineValidIn = np.load(\"./data/wine-validation-input.npy\")\n",
    "wineValidOut = np.load(\"./data/wine-validation-output.npy\")\n",
    "\n",
    "# correct the format of the data for use in KNN\n",
    "traindata = np.concatenate((wineTrainOut,wineTrainIn),1)\n",
    "testdata = np.concatenate((wineValidOut,wineValidIn),1)\n",
    "\n",
    "# set the number of neighbours for classification\n",
    "K = 3 \n",
    "\n",
    "# do the KNN\n",
    "expclass, CM = KNN(traindata, testdata, K)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: PCA-KNN classification on wine data\n",
    "<ul>\n",
    "  <li>Modify the value of $K$ and compare the results, the randomization should be fixed to have a fair comparison. </li>\n",
    "  <li>Apply PCA on the input data, and take the first three Principal Components (PCs), correspond to the highest variance of PCs. Include the first three PCs as input to KNN algorithm (instead of the original data). Compare the confusion matrix!</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
